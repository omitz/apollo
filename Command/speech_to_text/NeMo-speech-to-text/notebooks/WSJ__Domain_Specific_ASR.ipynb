{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Specific ASR: Wall Street Journal (WSJ) use case\n",
    "\n",
    "In this notebook we exemplify the end-to-end workflow of ASR domain adaptation using the `Domain Specific NeMo ASR application`. We start with an acoustic model pre-trained on open-source English datasets **[LibriSpeech](http://www.openslr.org/12)** and [English - Mozilla Common Voice](https://voice.mozilla.org/en/datasets) and fine-tune with **Wall Street Journal (WSJ)** news dataset. Next, to further improve domain performance, we train language model based on prefix beam search [KenLM]( https://github.com/kpu/kenlm) which imposes a language model constraint on the new predicted character based on previous (most probable) prefixes. The language Model we train is the [Baidu’s CTC decoder with N-Gram LM implementation](https://github.com/PaddlePaddle/DeepSpeech).\n",
    "\n",
    "Through this example case, we show we can easily do transfer learning or domain adaptation from [relatively old] fiction books (LibriSpeech) to [relatively modern] business news (WSJ).\n",
    "\n",
    "The steps followed in this notebook are:\n",
    "1. Preparations: Download Pre-trained model, Create Project and Datasets preparation\n",
    "2. Train (fine-tune) Acoustic Model\n",
    "3. Train Language Model\n",
    "4. Inference with Pre-trained and Fine-tuned models\n",
    "5. Compare models\n",
    "6. Export model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "# set directory to save results to\n",
    "import os\n",
    "if os.environ.get(\"DATA_DIR\") == None:\n",
    "    os.environ[\"DATA_DIR\"] = \"/data\"\n",
    "data_dir = os.environ.get(\"DATA_DIR\")\n",
    "print(data_dir)\n",
    "\n",
    "# required imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from tools.System.config import cfg\n",
    "from tools.System.reader import Reader\n",
    "from tools.System.nemo_fns import get_onnx_cmd, get_onnx_trt_cmd\n",
    "from tools.filetools import file_exists\n",
    "from tools.misc import create_lm_dataset, parse_manifest_wer, barplot_manifest, get_transcript, get_gtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Download Pre-trained model from NGC\n",
    "**The pre-trained model is already provided inside the container for you and loaded into the project's manifest by default.**\n",
    "\n",
    "Download pre-trained model from NGC. There exists multiple pre-trained models in [NGC](https://ngc.nvidia.com/catalog/models?orderBy=modifiedDESC&query=nemo&quickFilter=models&filters=), select the pre-trained Jasper or QuartzNet model of your preference. \n",
    "\n",
    "In this example we use:\n",
    "### LibriSpeech and Common Voice QuartzNet 15x5 for NeMo\n",
    "https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5\n",
    "\n",
    "QuarzNet is a Jasper-like network which uses separable convolutions and larger filter sizes. It has comparable accuracy to Jasper while having much fewer parameters. This particular model has 15 blocks each repeated 5 times.\n",
    "\n",
    "QuartzNet15x5 Encoder and Decoder neural module's checkpoints available here are trained using Neural Modules (NeMo) toolkit. NVIDIA’s Apex/Amp O1 optimization level was used for training on 8xV100 GPUs. These modules were trained using LibriSpeech (+-10% speed perturbation) and Mozilla's EN Common Voice \"validated\" set. \n",
    "\n",
    "Steps:\n",
    "- Download pre-trained model from NGC\n",
    "- unzip model inside the example_data directory\n",
    "\n",
    "Other pre-trained models:\n",
    "### Multidataset QuartzNet 15x5 for NeMo\n",
    "https://ngc.nvidia.com/catalog/models/nvidia:multidataset_quartznet15x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create a new project\n",
    "\n",
    "We create a project that will keep track of the datasets, configurations and models created across the acoustic model and language model workflows.\n",
    "Everything related to a project is saved in disc in a manifest that can be access through its `project_id`.\n",
    "\n",
    "At the start of the project, the manifest is pre-populated with the baseline acoustic model - a model pretrained on LibriSpeech and using greedy decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config file: /data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml\n",
      "Inference config file: /data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml\n",
      "Manifest is saved /data/results/manifests/WSJ-test_manifest.json \n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_id = 'WSJ-test'\n",
    "\n",
    "project = Reader.new(project_id)\n",
    "#project.manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Acoustic Dataset\n",
    "\n",
    "In this example use-case, we fine-tune the pre-trained acoustic and language models with Wall Street Journal news datasets. \n",
    "\n",
    "This dataset is part of the Linguistic Data Consortium and can be found here:\n",
    "- CSR-I (WSJ0) Complete: https://catalog.ldc.upenn.edu/LDC93S6A\n",
    "- CSR-II (WSJ1) Complete: https://catalog.ldc.upenn.edu/LDC94S13A\n",
    "\n",
    "Note: To download this dataset a license is required please refer to [LDC to learn more](https://www.ldc.upenn.edu/language-resources/data/obtaining). \n",
    "\n",
    "### Create NeMo ready - Acoustic Model Dataset\n",
    "NeMo requires datasets to be in the format of:\n",
    "- `wav` audio clips with sampling rate (16000) and max clip duration (16.5) specified by the [configuration file](/tools/NeMo/example_configs/quartznet15x5.yaml).\n",
    "- The dataset format as a `json` file where each entry has the keys: `audio_filepath`, `duration` and `text`.\n",
    "\n",
    "You can see the script used to create NeMo datasets from common_voice datasets in `tools/NeMo/create_common_voice_dataset.py`. This script can help you correctly format your audio clips and json training dataset. As well, you must normalize the text, i.e. lowercase text, remove punctuations and change digits to text representation. We provide utility functions inside the tools folder, e.g. `tools/transcript_tools.py`, that can help you with dataset preparation.\n",
    "\n",
    "After you create your dataset, make sure that the pre-processed dataset has the correct paths to the audio files. You can use the following commands to check and fix the paths.\n",
    "\n",
    "**Best practice: To obtain a better performing model you can use speed-perturbation (+-10%) to augment your dataset. To do this you can use tools like sox.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"audio_filepath\": \"/data/wsj/csr_2_comp/13-1.1/wsj1/si_tr_s/460/460c0201.wav\", \"duration\": 7.178375, \"text\": \"earlier this week defense secretary caspar weinberger told reporters at a breakfast meeting that the b one's problems weren't serious\"}\n"
     ]
    }
   ],
   "source": [
    "# check audio_filepath exists\n",
    "!head -n1 /data/datasets/wsj-train-si284-speed-0.9-1.1.json\n",
    "#!head -n1 /data/datasets/wsj-eval-92.json\n",
    "#!head -n1 /data/datasets/wsj-dev-93.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert file_exists(\"/data/wsj/csr_2_comp/13-1.1/wsj1/si_tr_s/460/460c0201.wav\"), \"file does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If incorrect path replace this path with the correct path inside the container:\n",
    "#!sed -i 's,/data/example_data,,g' /data/datasets/wsj-train-si284-speed-0.9-1.1.json\n",
    "#!sed -i 's,/data/example_data,,g' /data/datasets/wsj-eval-92.json\n",
    "#!sed -i 's,/data/example_data,,g' /data/datasets/wsj-dev-93.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm change\n",
    "#!head -n1 /data/datasets/wsj-train-si284-speed-0.9-1.1.json\n",
    "#!head -n1 /data/datasets/wsj-eval-92.json\n",
    "#!head -n1 /data/datasets/wsj-dev-93.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Language Model dataset\n",
    "You can create a languague model dataset from the transcripts (labels) of your acoustic training dataset. You can use the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lm = os.path.join(\"datasets\", \"wsj-train-si284.json\") # input acoustic dataset\n",
    "#train_lm = os.path.join(\"datasets\", \"wsj-lm-train-si284.txt\") # name of output lm dataset\n",
    "#create_lm_dataset(data_lm, train_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Acoustic Model - Transfer Learning with NeMo\n",
    "In the next steps we finetune a baseline model with our own data.\n",
    "\n",
    "The steps are: \n",
    "1. Add pre-processed dataset to project\n",
    "2. Adjust training parameters\n",
    "3. Generate training command\n",
    "4. Perform Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Add Training Datasets to the Project \n",
    "To train an acoustic model you require a pre-processed `json` training dataset.\n",
    "NeMo expects the dataset as a `json` file where each entry has the keys: `audio_filepath`, `duration` and `text`.\n",
    "\n",
    "**Here is where you can add your own domain specific acoustic dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset_name': 'WSJ-wsj-train-speed',\n",
       " 'train_dataset': '/data/datasets/wsj-train-si284-speed-0.9-1.1.json',\n",
       " 'finetuned_model_path': None,\n",
       " 'train_cmd': None,\n",
       " 'infer_cmd': None,\n",
       " 'train_params': {'num_gpus': 1,\n",
       "  'batch_size': 16,\n",
       "  'num_epochs': 1,\n",
       "  'lr': None,\n",
       "  'warmup_steps': None,\n",
       "  'weight_decay': None,\n",
       "  'model_config': '/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml',\n",
       "  'optimizer': 'novograd',\n",
       "  'amp_opt_level': 'O1',\n",
       "  'beta1': None,\n",
       "  'beta2': None,\n",
       "  'finetune': True,\n",
       "  'load_encoder': '/tmp/nemo_asr_app/models/quartznet15x5/JasperEncoder-STEP-247400.pt',\n",
       "  'load_decoder': '/tmp/nemo_asr_app/models/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt',\n",
       "  'work_dir': '/data/results/models/acoustic_models/WSJ-test'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train dataset\n",
    "train_json = \"/data/datasets/wsj-train-si284-speed-0.9-1.1.json\"\n",
    "assert file_exists(train_json), \"Train dataset file does NOT exist.\"\n",
    "\n",
    "project.add_dataset(train_json, \"WSJ-wsj-train-speed\", dataset_type=\"am-train\")\n",
    "project.manifest.am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Modify acoustic model training parameters\n",
    "The default training parameters are specified in the project manifest, training script and the configuration file  `[DATA_DIR]/results/config_files/]PROJCECT]_acoustic_quartznet15x5.yaml`. However, you can modify some of these parameters as follows.\n",
    "\n",
    "To see the full list of parameters accessible by the manifest see `project.manifest.am.train_params`, to see other parameters look at the NeMo training script at `/tools/NeMo/jasper_train.py`.\n",
    "\n",
    "Important Notes:\n",
    "- The parameter `amp_opt_level` set to O1 or above, enables NVIDIA's [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision).\n",
    "- To enable finetuning you must set the parameter `finetuning` to \"True\" and set the path to the `pre-trained encoder and decoder` to the parameters `load_encoder` and `load_decoder` respectively. Note, the finetuning and pre-trained encoder and decoders are already added at the start of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset_name': 'WSJ-wsj-train-speed',\n",
       " 'train_dataset': '/data/datasets/wsj-train-si284-speed-0.9-1.1.json',\n",
       " 'finetuned_model_path': None,\n",
       " 'train_cmd': None,\n",
       " 'infer_cmd': None,\n",
       " 'train_params': {'num_gpus': 4,\n",
       "  'batch_size': 32,\n",
       "  'num_epochs': 1,\n",
       "  'lr': 0.0001,\n",
       "  'warmup_steps': 0,\n",
       "  'weight_decay': None,\n",
       "  'model_config': '/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml',\n",
       "  'optimizer': 'novograd',\n",
       "  'amp_opt_level': 'O1',\n",
       "  'beta1': None,\n",
       "  'beta2': None,\n",
       "  'finetune': True,\n",
       "  'load_encoder': '/tmp/nemo_asr_app/models/quartznet15x5/JasperEncoder-STEP-247400.pt',\n",
       "  'load_decoder': '/tmp/nemo_asr_app/models/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt',\n",
       "  'work_dir': '/data/results/models/acoustic_models/WSJ-test'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set training parameters \n",
    "project.set_am_batch_size(32)\n",
    "project.set_am_num_gpus(4)\n",
    "project.set_am_learning_rate(0.0001)\n",
    "project.manifest.am.train_params.warmup_steps = 0 \n",
    "#project.manifest.am.train_params.weight_decay = 0.0001\n",
    "project.manifest.am.train_params.amp_opt_level = 'O1'\n",
    "\n",
    "# If your dataset has 200+ Hrs of audio you can fine-tune for less epochs, e.g. 100.\n",
    "#project.set_am_num_epochs(100)\n",
    "project.set_am_num_epochs(1)\n",
    "\n",
    "# novograd\n",
    "#project.manifest.am.train_params.beta1 = 0.95\n",
    "#project.manifest.am.train_params.beta2 = 0.25 \n",
    "\n",
    "# To modify the path to config file and the pretrained model\n",
    "#project.load_am_config_file(config_file='')\n",
    "#project.set_am_pretrained_model(model='')\n",
    "\n",
    "project.save_manifest()\n",
    "project.manifest.am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generate Training Command\n",
    "Next we generate the acoustic model training command, where we use the settings specified in the manifest to create the appropriate command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python -m torch.distributed.launch --nproc_per_node=4 /tmp/nemo_asr_app/tools/NeMo/jasper_train.py --batch_size=32 --num_epochs=1 --lr=0.0001 --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --optimizer=novograd --amp_opt_level=O1 --finetune --load_encoder=/tmp/nemo_asr_app/models/quartznet15x5/JasperEncoder-STEP-247400.pt --load_decoder=/tmp/nemo_asr_app/models/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt --work_dir=/data/results/models/acoustic_models/WSJ-test --exp_name=WSJ-test_finetuning --train_dataset=/data/datasets/wsj-train-si284-speed-0.9-1.1.json\n"
     ]
    }
   ],
   "source": [
    "am_training_cmd = project.get_am_train_cmd()\n",
    "print(\"! \"+ am_training_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Finetune Pretrained Jasper Model\n",
    "Now that we have the training command that reflects our project settings, we can perform acoustic model training. Note, acoustic model training can take a long time so it is better to run training directly from the terminal of the container, using the command generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if you completed the training before, you'll have checkpoints saved in the working directory.  We'll need to remove these to restart training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean log directory to re-run the training command\n",
    "# project.clean_am_workdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a Language Model\n",
    "\n",
    "Up to now, the transcript the ASR system generates is through an “end-to-end” [CTC-based network]( https://www.cs.toronto.edu/~graves/icml_2006.pdf) which matches audio and text without additional alignment information. However, ambiguities in the transcription, for example when collapsing repeated characters and removing blanks, can exist as the CTC-based network has little prior linguistic knowledge. This is where a language model comes in, as it can help solve those decoding ambiguities.\n",
    "\n",
    "Specifically, the decoder output or transcript, when introducing a language model, is dependent on both the CTC network (softmax) output and the language model prediction. The language model we use is based on prefix beam search [KenLM]( https://github.com/kpu/kenlm) which imposes a language model constraint on the new predicted character based on previous (most probable) prefixes. To learn more see [First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs](https://arxiv.org/pdf/1408.2873.pdf).\n",
    "\n",
    "In this notebook we train the [Baidu’s CTC decoder with N-Gram LM implementation](https://github.com/PaddlePaddle/DeepSpeech).\n",
    "\n",
    "The steps are: 1. Add pre-processed dataset to project, 2. Edit training parameters 3. Generate training command and perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Add LM Dataset to project\n",
    "\n",
    "To train a language model you require the pre-processed training text as a single column (per sentence) saved as a .txt file. **Here is where you can add your own domain specific dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset_name': 'LM-train',\n",
       " 'train_dataset': '/data/datasets/wsj-lm-data-small.txt',\n",
       " 'ngram': 6,\n",
       " 'finetuned_model': '/data/results/models/language_models/WSJ-test_lm.binary',\n",
       " 'train_cmd': None,\n",
       " 'infer_cmd': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lm =  \"/data/datasets/wsj-lm-data-small.txt\"\n",
    "assert file_exists(train_lm), \"Train dataset file does not exist.\"\n",
    "\n",
    "project.add_dataset(train_lm, \"LM-train\", dataset_type=\"lm-train\")\n",
    "project.manifest.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Edit Training Parameters\n",
    "We first edit language model training parameters. Here the only parameter to edit is the length of the sequence of words `(N)` used by the N-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset_name': 'LM-train',\n",
       " 'train_dataset': '/data/datasets/wsj-lm-data-small.txt',\n",
       " 'ngram': 6,\n",
       " 'finetuned_model': '/data/results/models/language_models/WSJ-test_lm.binary',\n",
       " 'train_cmd': None,\n",
       " 'infer_cmd': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edit parameters\n",
    "project.set_n_gram(6) # number of words for N-Gram\n",
    "project.manifest.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training Language Model\n",
    "The following function will generate the language model training command for our dataset. You can run this command directly in this notebook or in the container terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /tmp/nemo_asr_app/tools/NeMo/build_lm.py /data/datasets/wsj-lm-data-small.txt --n=6 --project_id=WSJ-test\n"
     ]
    }
   ],
   "source": [
    "lm_train_cmd = project.get_lm_train_cmd()\n",
    "print (\"! \" + lm_train_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean log directory to re-run the training \n",
    "#project.clean_lm_logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference with Pre-trained and Fine-tuned Models\n",
    "Next, we perform inference with both the pre-trained and fine-tuned models.\n",
    "\n",
    "The steps are: \n",
    "1. Add pre-processed evaluation datasets to project\n",
    "2. Inference using: Pre-trained model (Greedy Decoder)\n",
    "3. Inference using: Finetuned acoustic model (Greedy Decoder)\n",
    "4. Inference using: Finetuned acoustic model with added Language model\n",
    "\n",
    "At each inference case, we follow 3 steps: 1) Adjust inference parameters 2) Generate inference command 3) Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Add Inference Evaluation Datasets to the Project \n",
    "To perform inference you require a preprocessed `json` dataset, with the columns: `audio_filepath`, `duration` and `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj-eval']\n",
      "['/data/datasets/wsj-eval-92.json']\n"
     ]
    }
   ],
   "source": [
    "# add inference dataset\n",
    "eval_json=\"/data/datasets/wsj-eval-92.json\"\n",
    "#eval_json=\"data/datasets/wsj-dev-93.json\"\n",
    "assert file_exists(eval_json), \"Evaluation dataset file does not exist.\"\n",
    "\n",
    "project.add_dataset(eval_json, \"wsj-eval\", dataset_type=\"eval\")\n",
    "\n",
    "# Becuase we already added the evaluation dataset to project when doing the AM workflow we don't need to do it again\n",
    "print(project.manifest.eval_dataset_names)\n",
    "print(project.manifest.eval_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Inference with Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 4.2.1 Modify inference parameters\n",
    "The default inference parameters are specified in the project manifest, inference script and the configuration file. However, you can modify some of these parameters as follows.\n",
    "\n",
    "To see the full list of parameters accessible by the manifest see `project.manifest.inference_params`, to see other parameters look at the NeMo inference script at `/tools/NeMo/jasper_infer.py`.\n",
    "\n",
    "Important Notes:\n",
    "- The parameter `amp_opt_level` set to O1 or above, enables NVIDIA's [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision).\n",
    "\n",
    "- When `lm_path` is set to \"None\", the system will use a `greedy decoder`, and thus the language model parametrs `beam_width`, `alpha`, `beta` will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'load_dir': '/tmp/nemo_asr_app/models/quartznet15x5',\n",
       " 'model_config': '/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml',\n",
       " 'batch_size': 32,\n",
       " 'amp_opt_level': 'O1',\n",
       " 'save_results': '/data/results/inferences/WSJ-test',\n",
       " 'lm_path': None,\n",
       " 'beam_width': None,\n",
       " 'alpha': None,\n",
       " 'beta': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-trained model already loaded from the models folder\n",
    "project.manifest.inference_params.batch_size = 32\n",
    "project.manifest.inference_params.amp_opt_level = 'O1'\n",
    "\n",
    "project.save_manifest()\n",
    "project.manifest.inference_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Generate Inference Command: Pre-trained Model\n",
    "We generate the inference command using the manifest configurations and define a name to identify the model we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /tmp/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/tmp/nemo_asr_app/models/quartznet15x5 --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O1 --save_results=/data/results/inferences/WSJ-test --eval_datasets=/data/datasets/wsj-eval-92.json --model_id=Pretrained-greedy_decoder\n"
     ]
    }
   ],
   "source": [
    "# Model Id - We use this to identify the results of this model\n",
    "model_id = \"Pretrained-greedy_decoder\"\n",
    "\n",
    "# Inference with Pre-trained model\n",
    "am_infer_cmd = project.get_inference_cmd(model_id=model_id)\n",
    "print(\"! \"+ am_infer_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Run Inference with Pre-trained Model\n",
    "To run inference, copy the command generated above and paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nemo/collections/asr/audio_preprocessing.py:48: UserWarning: Could not import torchaudio. Some features might not work.\n",
      "  warnings.warn('Could not import torchaudio. Some features might not work.')\n",
      "[NeMo I 2020-04-09 22:22:35 collections:138] Dataset loaded with 333 files totalling 0.71 hours\n",
      "[NeMo I 2020-04-09 22:22:35 collections:139] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2020-04-09 22:22:35 jasper_eval:109] Evaluating 333 examples\n",
      "[NeMo I 2020-04-09 22:22:35 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-09 22:22:35 features:152] STFT using conv\n",
      "[NeMo I 2020-04-09 22:22:37 jasper_eval:122] ================================\n",
      "[NeMo I 2020-04-09 22:22:37 jasper_eval:124] Number of parameters in encoder: 18894656\n",
      "[NeMo I 2020-04-09 22:22:37 jasper_eval:126] Number of parameters in decoder: 29725\n",
      "[NeMo I 2020-04-09 22:22:37 jasper_eval:128] Total number of parameters: 18924381\n",
      "[NeMo I 2020-04-09 22:22:37 jasper_eval:130] ================================\n",
      "[NeMo I 2020-04-09 22:22:37 actions:1454] Restoring JasperEncoder from /tmp/nemo_asr_app/models/quartznet15x5/JasperEncoder-STEP-247400.pt\n",
      "[NeMo I 2020-04-09 22:22:38 actions:1454] Restoring JasperDecoderForCTC from /tmp/nemo_asr_app/models/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "[NeMo I 2020-04-09 22:22:38 actions:728] Evaluating batch 0 out of 11\n",
      "[NeMo I 2020-04-09 22:22:39 actions:728] Evaluating batch 1 out of 11\n",
      "[NeMo I 2020-04-09 22:22:40 actions:728] Evaluating batch 2 out of 11\n",
      "[NeMo I 2020-04-09 22:22:40 actions:728] Evaluating batch 3 out of 11\n",
      "[NeMo I 2020-04-09 22:22:40 actions:728] Evaluating batch 4 out of 11\n",
      "[NeMo I 2020-04-09 22:22:40 actions:728] Evaluating batch 5 out of 11\n",
      "[NeMo I 2020-04-09 22:22:41 actions:728] Evaluating batch 6 out of 11\n",
      "[NeMo I 2020-04-09 22:22:41 actions:728] Evaluating batch 7 out of 11\n",
      "[NeMo I 2020-04-09 22:22:41 actions:728] Evaluating batch 8 out of 11\n",
      "[NeMo I 2020-04-09 22:22:42 actions:728] Evaluating batch 9 out of 11\n",
      "[NeMo I 2020-04-09 22:22:42 actions:728] Evaluating batch 10 out of 11\n",
      "[NeMo I 2020-04-09 22:22:42 jasper_eval:158] Greedy WER 10.01%\n",
      "[NeMo I 2020-04-09 22:22:42 jasper_eval:241] Saving inference results to /data/results/inferences/WSJ-test/results-am_grid__wsj-eval-92__Pretrained-greedy_decoder.json\n"
     ]
    }
   ],
   "source": [
    "! python /tmp/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/tmp/nemo_asr_app/models/quartznet15x5 --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O1 --save_results=/data/results/inferences/WSJ-test --eval_datasets=/data/datasets/wsj-eval-92.json --model_id=Pretrained-greedy_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Inference with Finetuned model (Greedy Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 4.3.1 Modify inference parameters\n",
    "To use the fintuned acoustic model you **must** modify:\n",
    "- The `project.manifest.inference_params.load_dir` to use the fine-tuned model not the default pretrained model.\n",
    "    - The path to the finetuned model can be found at `project.manifest.am.finetuned_model_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'load_dir': '/tmp/nemo_asr_app/models/wsj_finetuned',\n",
       " 'model_config': '/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml',\n",
       " 'batch_size': 32,\n",
       " 'amp_opt_level': 'O1',\n",
       " 'save_results': '/data/results/inferences/WSJ-test',\n",
       " 'lm_path': None,\n",
       " 'beam_width': None,\n",
       " 'alpha': None,\n",
       " 'beta': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference with Fintuned model\n",
    "# We provide the finetuned model inside the models folder\n",
    "project.manifest.inference_params.load_dir = os.path.join(os.environ['APP_DIR'], 'models', 'wsj_finetuned') \n",
    "# But if you fintuned the model, the path is available in project.manifest.am.finetuned_model_path\n",
    "#project.manifest.inference_params.load_dir = project.manifest.am.finetuned_model_path\n",
    "\n",
    "project.manifest.inference_params.amp_opt_level = 'O1'\n",
    "\n",
    "project.save_manifest()\n",
    "project.manifest.inference_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Generate Inference Command\n",
    "We generate the inference command using the manifest configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /tmp/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/tmp/nemo_asr_app/models/wsj_finetuned --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O1 --save_results=/data/results/inferences/WSJ-test --eval_datasets=/data/datasets/wsj-eval-92.json --model_id=Finetuned_WSJ-greedy_decoder\n"
     ]
    }
   ],
   "source": [
    "# Model Id - We use this to identify the results of this model\n",
    "model_id = \"Finetuned_WSJ-greedy_decoder\"\n",
    "\n",
    "# Inference with Fine-tuned model\n",
    "am_infer_cmd = project.get_inference_cmd(model_id=model_id)\n",
    "print(\"! \"+ am_infer_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Run Inference with Fine-tuned Model\n",
    "To run inference, copy the command generated above and paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nemo/collections/asr/audio_preprocessing.py:48: UserWarning: Could not import torchaudio. Some features might not work.\n",
      "  warnings.warn('Could not import torchaudio. Some features might not work.')\n",
      "[NeMo I 2020-04-09 22:22:46 collections:138] Dataset loaded with 333 files totalling 0.71 hours\n",
      "[NeMo I 2020-04-09 22:22:46 collections:139] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2020-04-09 22:22:46 jasper_eval:109] Evaluating 333 examples\n",
      "[NeMo I 2020-04-09 22:22:46 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-09 22:22:46 features:152] STFT using conv\n",
      "[NeMo I 2020-04-09 22:22:49 jasper_eval:122] ================================\n",
      "[NeMo I 2020-04-09 22:22:49 jasper_eval:124] Number of parameters in encoder: 18894656\n",
      "[NeMo I 2020-04-09 22:22:49 jasper_eval:126] Number of parameters in decoder: 29725\n",
      "[NeMo I 2020-04-09 22:22:49 jasper_eval:128] Total number of parameters: 18924381\n",
      "[NeMo I 2020-04-09 22:22:49 jasper_eval:130] ================================\n",
      "[NeMo I 2020-04-09 22:22:49 actions:1454] Restoring JasperEncoder from /tmp/nemo_asr_app/models/wsj_finetuned/JasperEncoder-STEP-174000.pt\n",
      "[NeMo I 2020-04-09 22:22:49 actions:1454] Restoring JasperDecoderForCTC from /tmp/nemo_asr_app/models/wsj_finetuned/JasperDecoderForCTC-STEP-174000.pt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "[NeMo I 2020-04-09 22:22:49 actions:728] Evaluating batch 0 out of 11\n",
      "[NeMo I 2020-04-09 22:22:51 actions:728] Evaluating batch 1 out of 11\n",
      "[NeMo I 2020-04-09 22:22:51 actions:728] Evaluating batch 2 out of 11\n",
      "[NeMo I 2020-04-09 22:22:51 actions:728] Evaluating batch 3 out of 11\n",
      "[NeMo I 2020-04-09 22:22:52 actions:728] Evaluating batch 4 out of 11\n",
      "[NeMo I 2020-04-09 22:22:52 actions:728] Evaluating batch 5 out of 11\n",
      "[NeMo I 2020-04-09 22:22:52 actions:728] Evaluating batch 6 out of 11\n",
      "[NeMo I 2020-04-09 22:22:52 actions:728] Evaluating batch 7 out of 11\n",
      "[NeMo I 2020-04-09 22:22:53 actions:728] Evaluating batch 8 out of 11\n",
      "[NeMo I 2020-04-09 22:22:53 actions:728] Evaluating batch 9 out of 11\n",
      "[NeMo I 2020-04-09 22:22:53 actions:728] Evaluating batch 10 out of 11\n",
      "[NeMo I 2020-04-09 22:22:54 jasper_eval:158] Greedy WER 4.52%\n",
      "[NeMo I 2020-04-09 22:22:54 jasper_eval:241] Saving inference results to /data/results/inferences/WSJ-test/results-am_grid__wsj-eval-92__Finetuned_WSJ-greedy_decoder.json\n"
     ]
    }
   ],
   "source": [
    "! python /tmp/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/tmp/nemo_asr_app/models/wsj_finetuned --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O1 --save_results=/data/results/inferences/WSJ-test --eval_datasets=/data/datasets/wsj-eval-92.json --model_id=Finetuned_WSJ-greedy_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Inference with Fine-tuned model and Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Modify inference parameters\n",
    "\n",
    "In this section we **need to** set the LM parameters to enable inference with the beam-search decoder, specifically the trained LM model.\n",
    "\n",
    "The parameters to change are:\n",
    "- `lm_path`: Pointing to the `binary` file of the LM model you wish to use. The function `project.get_path_to_lm()` will return the path project's language model.\n",
    "- `beam_width`: determining how many prefixes (beams) will be selected at each timestep.\n",
    "- `alpha`: the weight of the language model.\n",
    "- `beta`: compensation term or word insertion weight. Required to control length of candidates (balance number of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'load_dir': '/tmp/nemo_asr_app/models/wsj_finetuned',\n",
       " 'model_config': '/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml',\n",
       " 'batch_size': 32,\n",
       " 'amp_opt_level': 'O1',\n",
       " 'save_results': '/data/results/inferences/WSJ-test',\n",
       " 'lm_path': '/tmp/nemo_asr_app/models/lm/WSJ_lm.binary',\n",
       " 'beam_width': 200,\n",
       " 'alpha': 3.0,\n",
       " 'beta': 0.1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fintuned Acoustic model\n",
    "project.manifest.inference_params.load_dir = os.path.join(os.environ['APP_DIR'], 'models', 'wsj_finetuned') \n",
    "\n",
    "# Inference with language model\n",
    "# We provide the language model inside the models folder\n",
    "project.manifest.inference_params.lm_path = os.path.join(os.environ['APP_DIR'], 'models', 'lm', 'WSJ_lm.binary') \n",
    "\n",
    "# But if you trained the language model, the path is available at project.get_path_to_lm()\n",
    "#project.manifest.inference_params.lm_path =  project.get_path_to_lm()\n",
    "\n",
    "# beam parameters\n",
    "project.manifest.inference_params.beam_width = 200\n",
    "project.manifest.inference_params.alpha = 3.0\n",
    "project.manifest.inference_params.beta = 0.1\n",
    "\n",
    "# other parameters\n",
    "project.manifest.inference_params.batch_size = 32\n",
    "project.manifest.inference_params.amp_opt_level = 'O1'\n",
    "\n",
    "project.save_manifest()\n",
    "project.manifest.inference_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Generate Inference Command\n",
    "We generate the inference command using the manifest configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /tmp/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/tmp/nemo_asr_app/models/wsj_finetuned --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O1 --save_results=/data/results/inferences/WSJ-test --lm_path=/tmp/nemo_asr_app/models/lm/WSJ_lm.binary --beam_width=200 --alpha=3.0 --beta=0.1 --eval_datasets=/data/datasets/wsj-eval-92.json --model_id=Finetuned-WSJ-LM-WSJ\n"
     ]
    }
   ],
   "source": [
    "# Model Id - We use this to identify the results of this model\n",
    "model_id = \"Finetuned-WSJ-LM-WSJ\"\n",
    "\n",
    "# Inference with Fine-tuned acoustic model and language model\n",
    "lm_infer_cmd = project.get_inference_cmd(model_id=model_id)\n",
    "print(\"! \"+ lm_infer_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Run Inference with Finetuned Model with Language Model\n",
    "To run inference, copy the command generated above and paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nemo/collections/asr/audio_preprocessing.py:48: UserWarning: Could not import torchaudio. Some features might not work.\n",
      "  warnings.warn('Could not import torchaudio. Some features might not work.')\n",
      "[NeMo I 2020-04-09 22:22:58 collections:138] Dataset loaded with 333 files totalling 0.71 hours\n",
      "[NeMo I 2020-04-09 22:22:58 collections:139] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2020-04-09 22:22:58 jasper_eval:109] Evaluating 333 examples\n",
      "[NeMo I 2020-04-09 22:22:58 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-09 22:22:58 features:152] STFT using conv\n",
      "[NeMo I 2020-04-09 22:23:00 jasper_eval:122] ================================\n",
      "[NeMo I 2020-04-09 22:23:00 jasper_eval:124] Number of parameters in encoder: 18894656\n",
      "[NeMo I 2020-04-09 22:23:00 jasper_eval:126] Number of parameters in decoder: 29725\n",
      "[NeMo I 2020-04-09 22:23:00 jasper_eval:128] Total number of parameters: 18924381\n",
      "[NeMo I 2020-04-09 22:23:00 jasper_eval:130] ================================\n",
      "[NeMo I 2020-04-09 22:23:00 actions:1454] Restoring JasperEncoder from /tmp/nemo_asr_app/models/wsj_finetuned/JasperEncoder-STEP-174000.pt\n",
      "[NeMo I 2020-04-09 22:23:00 actions:1454] Restoring JasperDecoderForCTC from /tmp/nemo_asr_app/models/wsj_finetuned/JasperDecoderForCTC-STEP-174000.pt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "[NeMo I 2020-04-09 22:23:01 actions:728] Evaluating batch 0 out of 11\n",
      "[NeMo I 2020-04-09 22:23:02 actions:728] Evaluating batch 1 out of 11\n",
      "[NeMo I 2020-04-09 22:23:03 actions:728] Evaluating batch 2 out of 11\n",
      "[NeMo I 2020-04-09 22:23:03 actions:728] Evaluating batch 3 out of 11\n",
      "[NeMo I 2020-04-09 22:23:03 actions:728] Evaluating batch 4 out of 11\n",
      "[NeMo I 2020-04-09 22:23:03 actions:728] Evaluating batch 5 out of 11\n",
      "[NeMo I 2020-04-09 22:23:04 actions:728] Evaluating batch 6 out of 11\n",
      "[NeMo I 2020-04-09 22:23:04 actions:728] Evaluating batch 7 out of 11\n",
      "[NeMo I 2020-04-09 22:23:04 actions:728] Evaluating batch 8 out of 11\n",
      "[NeMo I 2020-04-09 22:23:04 actions:728] Evaluating batch 9 out of 11\n",
      "[NeMo I 2020-04-09 22:23:05 actions:728] Evaluating batch 10 out of 11\n",
      "[NeMo I 2020-04-09 22:23:05 jasper_eval:158] Greedy WER 4.52%\n",
      "[NeMo I 2020-04-09 22:23:05 jasper_eval:176] ================================\n",
      "[NeMo I 2020-04-09 22:23:05 jasper_eval:177] Infering with (alpha, beta): (3.0, 0.1)\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:202] Beam WER 2.20%\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:205] Beam WER for (alpha, beta)\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:206] ================================\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:207] \n",
      "    ((3.0, 0.1), 2.197412723728513)\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:208] ================================\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:210] Best (alpha, beta): (3.0, 0.1), WER: 2.20%\n",
      "[NeMo I 2020-04-09 22:23:16 jasper_eval:241] Saving inference results to /data/results/inferences/WSJ-test/results-lm_grid__wsj-eval-92__Finetuned-WSJ-LM-WSJ.json\n"
     ]
    }
   ],
   "source": [
    "! python /tmp/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/tmp/nemo_asr_app/models/wsj_finetuned --model_config=/data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O1 --save_results=/data/results/inferences/WSJ-test --lm_path=/tmp/nemo_asr_app/models/lm/WSJ_lm.binary --beam_width=200 --alpha=3.0 --beta=0.1 --eval_datasets=/data/datasets/wsj-eval-92.json --model_id=Finetuned-WSJ-LM-WSJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Models Comparison\n",
    "Next, we compare the Word Error Rates for both the fine-tuned model and pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Add Inference results to manifest\n",
    "We first add the results from our inference runs to the project's manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added results for model Pretrained-greedy_decoder - /data/datasets/wsj-eval-92.json.\n",
      "Added results for model Finetuned_WSJ-greedy_decoder - /data/datasets/wsj-eval-92.json.\n",
      "Added results for model Finetuned-WSJ-LM-WSJ - /data/datasets/wsj-eval-92.json.\n",
      "['/data/datasets/wsj-eval-92.json']\n"
     ]
    }
   ],
   "source": [
    "project.add_inference_results()\n",
    "inferences_all = list(project.manifest.inference.keys())\n",
    "inferences_all.sort()\n",
    "print(inferences_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Word Error Rate Comparison\n",
    "We plot WER performance of eval dataset and we also select the performance metric we wish to sort by.\n",
    "\n",
    "Note, if `lm_wer` is present we chose that value over the `wer` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pretrained-greedy_decoder': {'wer': 0.10012404749246855,\n",
       "  'path': '/data/results/inferences/WSJ-test/results-am_grid__wsj-eval-92__Pretrained-greedy_decoder.json',\n",
       "  'lm_wer': None},\n",
       " 'Finetuned_WSJ-greedy_decoder': {'wer': 0.045188729399255716,\n",
       "  'path': '/data/results/inferences/WSJ-test/results-am_grid__wsj-eval-92__Finetuned_WSJ-greedy_decoder.json',\n",
       "  'lm_wer': None},\n",
       " 'Finetuned-WSJ-LM-WSJ': {'wer': 0.045188729399255716,\n",
       "  'path': '/data/results/inferences/WSJ-test/results-lm_grid__wsj-eval-92__Finetuned-WSJ-LM-WSJ.json',\n",
       "  'lm_wer': 0.021974127237285132}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_inference = project.manifest.inference[inferences_all[0]]\n",
    "sel_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/datasets/wsj-eval-92.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inference_types</th>\n",
       "      <th>lm_wer</th>\n",
       "      <th>wer</th>\n",
       "      <th>percentWER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finetuned-WSJ-LM-WSJ</td>\n",
       "      <td>0.021974</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>2.197413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finetuned_WSJ-greedy_decoder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045189</td>\n",
       "      <td>4.518873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained-greedy_decoder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100124</td>\n",
       "      <td>10.012405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                inference_types    lm_wer       wer  percentWER\n",
       "2          Finetuned-WSJ-LM-WSJ  0.021974  0.045189    2.197413\n",
       "1  Finetuned_WSJ-greedy_decoder       NaN  0.045189    4.518873\n",
       "0     Pretrained-greedy_decoder       NaN  0.100124   10.012405"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_metric = 'percentWER'\n",
    "# Note if you wish to plot (and parse) only a specific set of models you can use the argument 'keep' and \n",
    "# list the model_id(s) you wish to keep.\n",
    "df = parse_manifest_wer(sel_inference, sort_metric=sort_metric)\n",
    "                           \n",
    "print(inferences_all[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the dataframe created by the `parse_manifest_wer` function to plot the performance of the different models.\n",
    "We use the function `barplot_manifest` that has the arguments `([dataframe],[metric to plot],[title],[xlabel],[ylabel])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "wer = barplot_manifest(df,'percentWER',\"Word Error Rate\", \"Inference type\",\"Percent %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('inference_types', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = df.loc['Pretrained-greedy_decoder','wer'] # pre-trained model\n",
    "new = df.loc['Finetuned-WSJ-LM-WSJ','lm_wer'] # fine-tuned am and lm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreased WER by 78.05%\n"
     ]
    }
   ],
   "source": [
    "per_decrease = 100*((org-new)/org)\n",
    "print(\"Decreased WER by {0:.2f}%\".format(per_decrease))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Deployment\n",
    "Now that you have created SOTA performance for your domain, you are ready to export your model to be deployed. \n",
    "\n",
    "The process to follow is:\n",
    "1. [NeMo](https://github.com/NVIDIA/NeMo) checkpoint -> ONNX\n",
    "2. ONNX -> [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt)\n",
    "\n",
    "After this process you will be ready to deploy your optimized model with [NVIDIA Jarvis]( https://developer.nvidia.com/nvidia-jarvis) or [TensorRT Inference Server](https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/).\n",
    "\n",
    "Definitions:\n",
    "- **ONNX** is a standard for representing deep learning models enabling them to be transferred between frameworks.\n",
    "- **TensorRT** enables the network to be compressed, optimized and deployed as a runtime without the overhead of a framework.\n",
    "- **NVIDIA Jarvis** is an SDK for building and deploying AI applications that fuse vision, speech and other sensors.\n",
    "- **NVIDIA TensortRT Inference Server** is an inference microservice for data center production that maximizes GPU utilization and seamlessly integrates into DevOps deployments with Docker and Kubernetes integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 NeMo model checkpoints to ONNX\n",
    "This process will convert the `.pt` model checkpoints into `onnx` format. After you run this process you should have two .onnx files - one for the encoder and one for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting fine-tuned model\n",
      "Encoder: /tmp/nemo_asr_app/models/wsj_finetuned/JasperEncoder-STEP-174000.pt\n",
      "Decoder /tmp/nemo_asr_app/models/wsj_finetuned/JasperDecoderForCTC-STEP-174000.pt\n"
     ]
    }
   ],
   "source": [
    "pretrained = False\n",
    "if pretrained:\n",
    "    # pre-trained model\n",
    "    config = project.manifest.am.train_params.model_config\n",
    "    encoder = project.manifest.am.train_params.load_encoder\n",
    "    decoder = project.manifest.am.train_params.load_decoder\n",
    "    onnx_encoder = os.path.join(cfg.MODEL.ONNX_MODELS,\"pre-trained_encoder.onnx\")\n",
    "    onnx_decoder = os.path.join(cfg.MODEL.ONNX_MODELS,\"pre-trained_decoder.onnx\")\n",
    "    print(\"Exporting pre-trained model\")\n",
    "    print(\"Encoder:\", encoder)\n",
    "    print(\"Decoder\", decoder)\n",
    "else:\n",
    "    # fine-tuned model\n",
    "    config = project.manifest.am.train_params.model_config\n",
    "    # if you wish to use the finetuned model you can use the encoder and decoder in: \n",
    "    # project.manifest.am.finetuned_model_path\n",
    "    encoder = os.path.join(cfg.NEMO.WSJ_FINETUNED,\"JasperEncoder-STEP-174000.pt\")\n",
    "    decoder = os.path.join(cfg.NEMO.WSJ_FINETUNED, \"JasperDecoderForCTC-STEP-174000.pt\")\n",
    "    onnx_encoder = os.path.join(cfg.MODEL.ONNX_MODELS,\"finetuned_encoder.onnx\")\n",
    "    onnx_decoder = os.path.join(cfg.MODEL.ONNX_MODELS,\"finetuned_decoder.onnx\")\n",
    "    print(\"Exporting fine-tuned model\")\n",
    "    print(\"Encoder:\", encoder)\n",
    "    print(\"Decoder\", decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /workspace/nemo/scripts/export_jasper_to_onnx.py --config /data/results/config_files/WSJ-test_acoustic_quartznet15x5.yaml --nn_encoder /tmp/nemo_asr_app/models/wsj_finetuned/JasperEncoder-STEP-174000.pt --nn_decoder /tmp/nemo_asr_app/models/wsj_finetuned/JasperDecoderForCTC-STEP-174000.pt --onnx_encoder /data/results/models/ONNX/finetuned_encoder.onnx --onnx_decoder /data/results/models/ONNX/finetuned_decoder.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_cmd = get_onnx_cmd(config, encoder, decoder, onnx_encoder, onnx_decoder)\n",
    "print(\"! \" + onnx_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 ONNX to TRT\n",
    "Next, we convert the ONNX model files to TensorRT to enable inference optimization that delivers low latency and high-throughput. In this process you are generating an engine that is optimized for a target platform.\n",
    "\n",
    "Note: You must convert both the Encoder and Decoder files. \n",
    "Learn more at: https://developer.nvidia.com/tensorrt\n",
    "\n",
    "Since the optimization is platform specific, if you wish to modify the model export parameters, look at the script: `/tmp/NeMo/scripts/export_jasper_onnx_to_trt.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Convert Decoder\n",
    "Run the following command to export the ONNX Decoder model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /workspace/nemo/scripts/export_jasper_onnx_to_trt.py /data/results/models/ONNX/finetuned_decoder.onnx /data/results/models/TRT/finetuned_decoder_trt_plan.engine\n"
     ]
    }
   ],
   "source": [
    "# decoder\n",
    "onnx_path = onnx_decoder\n",
    "if pretrained:\n",
    "    trt_plan = os.path.join(cfg.MODEL.TRT_MODELS,\"pre-trained_decoder_trt_plan.engine\")\n",
    "else:\n",
    "    # fine-tuned model\n",
    "    trt_plan = os.path.join(cfg.MODEL.TRT_MODELS,\"finetuned_decoder_trt_plan.engine\")\n",
    "trt_cmd = get_onnx_trt_cmd(onnx_path, trt_plan)\n",
    "print(\"! \" + trt_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Convert Encoder\n",
    "Run the following command to export the ONNX Encoder model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! python /workspace/nemo/scripts/export_jasper_onnx_to_trt.py /data/results/models/ONNX/finetuned_encoder.onnx /data/results/models/TRT/finetuned_encoder_trt_plan.engine\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "onnx_path = onnx_encoder\n",
    "if pretrained:\n",
    "    trt_plan = os.path.join(cfg.MODEL.TRT_MODELS,\"pre-trained_encoder_trt_plan.engine\")\n",
    "else:\n",
    "    # fine-tuned model\n",
    "    trt_plan = os.path.join(cfg.MODEL.TRT_MODELS,\"finetuned_encoder_trt_plan.engine\")\n",
    "trt_cmd = get_onnx_trt_cmd(onnx_path, trt_plan)\n",
    "print(\"! \" + trt_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
