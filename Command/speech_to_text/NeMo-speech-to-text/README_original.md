# Domain Specific – NeMo ASR Application
The Domain Specific - NeMo Automatic Speech Recognition (ASR) Application facilitates training, evaluation and performance comparison of ASR models. This NeMo application enables you to train or **fine-tune** pre-trained (acoustic and language) **ASR models** with your **own data**. Through this application, we empower you to create (on-prem) your own ASR models built for your domain specific data, that may be sensitive and thus not suitable for cloud ASR solutions. This application gives you the ability to progressively create better performing ASR models specifically built for your use case.

The Domain Specific - NeMo ASR application is a packaged and easy to use end-to-end ASR system that facilitates:
-   Acoustic Model training (and finetuning) on your own data.
-   Addition of a Language Model, as well as Language Model training on your own data.
-   Transcription (speech to text) with the models created.
-   Multi-model comparison that facilitates the understanding of performance progression.

You can learn more about this application from this [NVIDIA Developer Blog]( https://devblogs.nvidia.com/how-to-build-domain-specific-automatic-speech-recognition-models-on-gpus/) or download the [application container](https://ngc.nvidia.com/catalog/containers/nvidia:nemo_asr_app_img) from NGC.

We use the [NVIDIA Neural Modules (NeMo)](https://nvidia.github.io/NeMo/index.html) as the underlying ASR engine. NeMo is a toolkit for building Conversational AI applications. Through modular Deep Neural Networks (DNN) development, [NeMo](https://github.com/NVIDIA/NeMo) enables fast experimentation by connecting modules, mixing and matching components. *Neural Modules* are conceptual blocks of neural networks that take typed inputs and produce typed outputs, these typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. The toolkit comes with extendable collections of pre-built modules for automatic speech recognition (ASR), natural language processing (NLP) and text synthesis (TTS).

# Table of Contents
-   [Installation and Getting Started](#installation-and-getting-started)
-   [Domain ASR Application](#domain-asr-application)
-   [Automatic Speech Recognition Pipeline](#automatic-speech-recognition-pipeline)

# Installation and Getting Started
Getting stated with the `nemo_asr_app` is very simple. The latest version of the application is **v1.1**.

## Running from NGC container
You can obtain the application by downloading the container from NGC in this [link]( https://ngc.nvidia.com/catalog/containers/nvidia:nemo_asr_app_img). The image contains the complete Domain Specific NeMo ASR application (including notebooks, tools and scripts).

**1. Download the container from NGC**
``` bash
docker pull nvcr.io/nvstaging/nemo/nemo_asr_app_img:[latest tag]
```
**2. Run the application**

The application consists of a demo notebook and a notebook that walkthroughs the complete workflow of training and evaluating ASR models. If you wish to run the walkthrough notebook, you must set the directories for *results* (`DATA_DIR`) and the *dataset* (`WSJ_DIR`) for your specific setup.
```bash
export DATA_DIR="/raid/datasets/asr/data" 
export WSJ_DIR="/wsj"
```
To run the container, you can use the following `docker run command`. Note: if you are *just* running the demo notebook, you don’t need to mount the `$DATA_DIR` and `$WSJ_DIR`.
```bash
docker run --runtime=nvidia -it --rm --name run_nemo_asr_app_cont      \
 --ipc=host       \
--env DATA_DIR=$DATA_DIR     \
 -v $DATA_DIR:$DATA_DIR  \
 -v $WSJ_DIR:$WSJ_DIR      \
 -p 8888:8888       \
nvcr.io/nvstaging/nemo/nemo_asr_app_img:v1.1 \
jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' --NotebookApp.custom_display_url=http://$(ip route get 8.8.8.8 | head -1 | awk '{print $7}'):8888
```

## Run from Repository
Alternatively, you can clone the `nemo_asr_app` repository and follow the running instructions below.

### Running the application
You can run the application using the [`./run_app.sh` script](run_app.sh), which will prompt you for a `Data directory` and `Port number`. You can also input these as arguments when running the script:
```bash
./run_app.sh 8888 /data
```
- The `Data directory` will hold all the models, datasets and outputs generated by the tool and thus requires large storage (ASR models can take multiple Gigabytes). The data directory can be at any location within your system, and it will be automatically mounted inside the container by the [run_app.sh](run_app.sh) script.

- The `port` number will be used to access the application notebooks

The [run_app.sh](run_app.sh) script encapsulates a typical process of starting a container. It builds (if necessary) and runs a docker image based on a Dockerfile which is built on top of the [NeMo Toolkit container]( https://ngc.nvidia.com/catalog/containers/nvidia:nemo).

Note: when building/running the image from the [run_app.sh](run_app.sh) script, you can select to run it as root or your user by setting the `SUDO_USER` variable to 1 or 0 respectively. 

## Code Structure
This repository is divided into the following sub-folders each serving different functionalities.

* [models](models): This contains the pre-trained model and fine-tuned models. 
* [notebooks](notebooks): Application’s notebooks that demo and walk through the process of ASR model training and evaluation.
* [tools](tools): Scripts and tools that enable the system's functionality.
* [run_app.sh](run_app.sh) and [Dockerfile](Dockerfile): Scripts to run the application.

# Domain ASR Application
In our application, we enable a complete end-to-end workflow to enable domain adaptation of ASR models using your own data. This is done through the following steps:
1.	Preparations: Download Pre-trained model, Datasets and Create Project
2.	Train (fine-tune) Acoustic Model
3.	Train Language Model
4.	Inference with Pre-trained and Fine-tuned models
5.	Performance (Word Error Rate) comparison

We show we can easily do transfer learning or domain adaptation from [relatively old] fiction books (LibriSpeech) to [relatively modern] business news (WSJ). We use a pre-trained model [QuartzeNet 15x5]( https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5) found in NGC, which is pre-trained on open-source `English` datasets [LibriSpeech](http://www.openslr.org/12) and [English - Mozilla Common Voice](https://voice.mozilla.org/en/datasets). We fine tune this model with **Wall Street Journal (WSJ)** news dataset. To further improve domain performance, we train a language model based on prefix beam search [KenLM]( https://github.com/kpu/kenlm) which imposes a language model constraint on the new predicted character based on previous (most probable) prefixes.

## Project Tracking of Data, Models and Configurations
To simplify and enable reproducibility of the ASR workflow, our application allows you to we create a `project` which enables the tracking of datasets, models and configurations. Everything related to a project is saved in disc in a manifest that can be access through its `project_id`. At the start of the project, the manifest is pre-populated with the baseline pre-trained model. 

# Automatic Speech Recognition Pipeline
The Automatic Speech Recognition (ASR) NeMo model used in this application is based on a [Jasper]( https://arxiv.org/abs/1904.03288)-like model named [QuartzNet]( https://arxiv.org/pdf/1910.10261.pdf).  The QuartzNet model can achieve Jasper’s performance but with a lot less parameters (form about 333M to about 19M). This model consists of separable convolutions and larger filters, often denoted by QuartzNet_[BxR], where B is the number of blocks, and R - the number of convolutional sub-blocks within a block. Each sub-block contains a 1-D separable convolution, batch normalization, ReLU, and dropout. To learn more about NeMo’s ASR models see [tutorial](https://nvidia.github.io/NeMo/asr/models.html).

Jasper and QuartzNet are a [CTC-based]( https://www.cs.toronto.edu/~graves/icml_2006.pdf) end-to-end model, which can predict a transcript directly from an audio input, without additional alignment information.

If you wish to learn how the application builds the NeMo ASR engine, specifically the training and evaluation workflows, refer to the [jasper_train.py](/tools/NeMo/jasper_train.py) and [jasper_eval.py](/tools/NeMo/jasper_eval.py) scripts inside the tools folder.

### ASR-CTC pipeline
![CTC pipeline](https://miro.medium.com/max/4131/1*4ailcffDrQH3v9FquuX30A.png)
Image Source: [CTC Networks and Language Models: Prefix Beam Search Explained](https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306)

The typical ASR-CTC pipeline is shown in the Figure above. Here, the CTC network creates a probability CTC matrix, from the audio input, where columns represent a timestep and rows correspond to a letter in our alphabet, note the probabilities of each column (across all letters) sum to 1. For prediction using max decoding or greedy decoding, the letter with the highest probability at each timestep is chosen, in other words a temporal softmax output layer is used. Next, the repeated characters are removed or collapsed, and blank tokens are discarded. Additionally, a language model can be used to solve ambiguities in the in the transcription or softmax output, with the help of linguistic knowledge provided by prefix beam search. To learn more see [link]( https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306).

The CTC-ASR training pipeline followed by NeMo is shown in the following figure:
![NeMo CTC-ASR](https://nvidia.github.io/NeMo/_images/ctc_asr.png)
Image Source: [NVIDIA Neural Modules: NeMo](https://nvidia.github.io/NeMo/)

This includes:
-   audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)
-   neural acoustic model (which predicts a probability distribution P_t(c) over vocabulary characters c per each time step t given input features per each timestep)
-   CTC loss function
